{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST-Keras-NN\n",
    "\n",
    "The following script executes a program for digit recognition on the mnist database using Keras Multi-layer Perceptron classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dependencies\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "import time\n",
    "import os\n",
    "\n",
    "%matplotlib inline\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.datasets import mnist\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "import config\n",
    "\n",
    "### Configurations\n",
    "# Training-Size\n",
    "num_train = config.num_train                   # 60000 for full data set \n",
    "num_test  = config.num_test                    # 10000 for full data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple function to log information\n",
    "path = os.getcwd()+\"/log\"\n",
    "logDir = os.path.exists(path)\n",
    "if not logDir:\n",
    "    os.makedirs(path)\n",
    "\n",
    "training_results = path+\"/keras-nn-training-log.txt\"\n",
    "def log_training_results(*s):\n",
    "    with open(training_results, 'a') as f:\n",
    "        for arg in s:\n",
    "            print(arg, file=f)\n",
    "            print(arg)\n",
    "\n",
    "print(\"Generated data will be located in \", training_results)\n",
    "log_training_results(\"[%s] on (%s, %s) using (Train: %s, Test: %s)\" % (datetime.now(), config.os, config.cpu, config.num_train, config.num_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch MNIST-Data from Keras repository\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display (Train) (Test) datasets\n",
    "print(\"Shape of training data:\\t\\t\", X_train.shape)\n",
    "print(\"Shape of training labels:\\t\", y_train.shape)\n",
    "print(\"Shape of testing data:\\t\\t\", X_test.shape)\n",
    "print(\"Shape of testing labels:\\t\", y_test.shape)\n",
    "\n",
    "# i.e.: We have 60000 images with a size of 28x28 pixels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize some examples\n",
    "num_classes = 10 # 0 .. 9\n",
    "f, ax = plt.subplots(1, num_classes, figsize=(20,20))\n",
    "for i in range(0, num_classes):\n",
    "  sample = X_train[y_train == i][0]\n",
    "  ax[i].imshow(sample, cmap='gray')\n",
    "  ax[i].set_title(\"Label: {}\".format(i), fontsize=16)\n",
    "  ax[i].axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape the data such that we have access to every pixel of the image\n",
    "# The reason to access every pixel is that only then we can apply deep learning ideas and can assign color code to every pixel.\n",
    "train_data = X_train.reshape((X_train.shape[0], 28*28)).astype('float32')\n",
    "train_label = y_train.astype(\"float32\")\n",
    "\n",
    "test_data = X_test.reshape((X_test.shape[0], 28*28)).astype('float32')\n",
    "test_label = y_test.astype(\"float32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We know the RGB color code where different values produce various colors. It is also difficult to remember every color combination. \n",
    "# We already know that each pixel has its unique color code and also we know that it has a maximum value of 255. \n",
    "# To perform Machine Learning, it is important to convert all the values from 0 to 255 for every pixel to a range of values from 0 to 1.\n",
    "train_data = train_data / 255\n",
    "test_data = test_data / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Force the amount of columns to fit the necessary sizes required by the neural network\n",
    "train_label = keras.utils.to_categorical(train_label, num_classes)\n",
    "test_label = keras.utils.to_categorical(test_label, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As an optional step, we decrease the training and testing data size, such that the algorithms perform their execution in acceptable time\n",
    "train_data = train_data[1:num_train,]\n",
    "train_label = train_label[1:num_train]\n",
    "\n",
    "test_data = test_data[1:num_test,]\n",
    "test_label = test_label[1:num_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display (Train) (Test) datasets\n",
    "\n",
    "print(\"Reshaped training data:\\t\\t\", train_data.shape)\n",
    "print(\"Reshaped training labels:\\t\", train_label.shape)\n",
    "print(\"Reshaped testing data:\\t\\t\", test_data.shape)\n",
    "print(\"Reshaped testing labels:\\t\", test_label.shape)\n",
    "\n",
    "# As we can see: We now have X images with 784 pixels in total\n",
    "# We now operate on this data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-layer Perceptron classifier\n",
    "\n",
    "For more information, see: https://keras.io/guides/sequential_model/ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model: https://keras.io/guides/sequential_model/\n",
    "model = Sequential()\n",
    "\n",
    "# Create model layers\n",
    "model.add(Dense(units=128, input_shape=(784,), activation='relu'))\n",
    "model.add(Dense(units=128, activation='relu'))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(units=10, activation='softmax'))\n",
    "\n",
    "# Compile model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional learning configurations\n",
    "batch_size = 512\n",
    "epochs=10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "start_time = time.time()\n",
    "model.fit(x=train_data, y=train_label, batch_size=batch_size, epochs=epochs)\n",
    "end_time = time.time() - start_time\n",
    "\n",
    "params = {\"Keras\":{}}\n",
    "log_training_results(\"Trained new model: %s in %s seconds\" % (params, end_time))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model based on supplied tags\n",
    "start_time = time.time()\n",
    "test_loss, test_acc = model.evaluate(train_data, train_label)\n",
    "end_time = time.time() - start_time\n",
    "\n",
    "log_training_results(\"\\tPredicting train data -- execution time: %ss\" % (end_time))\n",
    "log_training_results(\"\\t[%s] -- Accuracy: %s; Loss: %s\" % (params, test_acc, test_loss))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model based on supplied tags\n",
    "start_time = time.time()\n",
    "test_loss, test_acc = model.evaluate(test_data, test_label)\n",
    "end_time = time.time() - start_time\n",
    "\n",
    "log_training_results(\"\\tPredicting test data --  execution time: %ss\" % (end_time))\n",
    "log_training_results(\"\\t[%s] -- Accuracy: %s; Loss: %s\" % (params, test_acc, test_loss))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let model predict data\n",
    "y_pred = model.predict(test_data)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "\n",
    "print(y_pred)\n",
    "print(y_pred_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View a correctly predicted datapoint\n",
    "random_idx = np.random.choice(len(test_data))\n",
    "x_sample = test_data[random_idx]\n",
    "y_true = np.argmax(test_label, axis=1)\n",
    "y_sample_true = y_true[random_idx]\n",
    "y_sample_pred_class = y_pred_classes[random_idx]\n",
    "\n",
    "plt.title(\"Predicted: {}, True: {}\".format(y_sample_pred_class, y_sample_true), fontsize=16)\n",
    "plt.imshow(x_sample.reshape(28, 28), cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize estimation over correct and incorrect prediction via confusion matrix\n",
    "confusion_mtx = confusion_matrix(y_true, y_pred_classes)\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots(figsize=(15,10))\n",
    "ax = sns.heatmap(confusion_mtx, annot=True, fmt='d', ax=ax, cmap=\"Blues\")\n",
    "ax.set_xlabel('Predicted Label')\n",
    "ax.set_ylabel('True Label')\n",
    "ax.set_title('Confusion Matrix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Review some Errors\n",
    "# Create some sets of data\n",
    "errors = (y_pred_classes - y_true != 0)\n",
    "y_pred_classes_errors = y_pred_classes[errors]\n",
    "y_pred_errors = y_pred[errors]\n",
    "y_true_errors = y_true[errors]\n",
    "x_test_errors = test_data[errors]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate error set results\n",
    "y_pred_errors_probability = np.max(y_pred_errors, axis=1)\n",
    "true_probability_errors = np.diagonal(np.take(y_pred_errors, y_true_errors, axis=1))\n",
    "diff_errors_pred_true = y_pred_errors_probability - true_probability_errors\n",
    "\n",
    "# Get list of indices of sorted differences\n",
    "sorted_idx_diff_errors = np.argsort(diff_errors_pred_true)\n",
    "top_idx_diff_errors = sorted_idx_diff_errors[-5:] # 5 last ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show error points which were the most failing ones\n",
    "num = len(top_idx_diff_errors)\n",
    "f, ax = plt.subplots(1, num, figsize=(30,30))\n",
    "\n",
    "for i in range(0, num):\n",
    "  idx = top_idx_diff_errors[i]\n",
    "  sample = x_test_errors[idx].reshape(28,28)\n",
    "  y_t = y_true_errors[idx]\n",
    "  y_p = y_pred_classes_errors[idx]\n",
    "  ax[i].imshow(sample, cmap='gray')\n",
    "  ax[i].set_title(\"Predicted label :{}\\nTrue label: {}\".format(y_p, y_t), fontsize=22)\n",
    "  ax[i].axis('off')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3e2b2785e650337f79381cd4c5df08c4d5dc4623a6a0d2da7e01465b331d0fcc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
