[2022-09-13 02:29:27.958105] on (Linux-5.15.0-46-generic-x86_64-with-glibc2.29, x86_64) using (Train: 15000, Test: 2500)
--- [2022-09-13 02:29:42.668173] Running Parameter-Tests [SKLEARN-NN] ---
	Best parameters set found on following development set:
		Support Vector: MLPClassifier(alpha=0.05, hidden_layer_sizes=(784,), learning_rate='adaptive')
		Support Vector Parametrization: {'activation': 'relu', 'alpha': 0.05, 'hidden_layer_sizes': (784,), 'learning_rate': 'adaptive', 'solver': 'adam'}
		Asserted Score: 0.9649977770367901
Total Score 		 Configurations
0.926 (+/-0.014)	{'activation': 'tanh', 'alpha': 0.0001, 'hidden_layer_sizes': (28, 28), 'learning_rate': 'constant', 'solver': 'sgd'}
0.938 (+/-0.009)	{'activation': 'tanh', 'alpha': 0.0001, 'hidden_layer_sizes': (28, 28), 'learning_rate': 'constant', 'solver': 'adam'}
0.930 (+/-0.011)	{'activation': 'tanh', 'alpha': 0.0001, 'hidden_layer_sizes': (28, 28), 'learning_rate': 'adaptive', 'solver': 'sgd'}
0.935 (+/-0.010)	{'activation': 'tanh', 'alpha': 0.0001, 'hidden_layer_sizes': (28, 28), 'learning_rate': 'adaptive', 'solver': 'adam'}
0.916 (+/-0.013)	{'activation': 'tanh', 'alpha': 0.0001, 'hidden_layer_sizes': (784,), 'learning_rate': 'constant', 'solver': 'sgd'}
0.953 (+/-0.004)	{'activation': 'tanh', 'alpha': 0.0001, 'hidden_layer_sizes': (784,), 'learning_rate': 'constant', 'solver': 'adam'}
0.915 (+/-0.013)	{'activation': 'tanh', 'alpha': 0.0001, 'hidden_layer_sizes': (784,), 'learning_rate': 'adaptive', 'solver': 'sgd'}
0.953 (+/-0.003)	{'activation': 'tanh', 'alpha': 0.0001, 'hidden_layer_sizes': (784,), 'learning_rate': 'adaptive', 'solver': 'adam'}
0.929 (+/-0.012)	{'activation': 'tanh', 'alpha': 0.05, 'hidden_layer_sizes': (28, 28), 'learning_rate': 'constant', 'solver': 'sgd'}
0.944 (+/-0.005)	{'activation': 'tanh', 'alpha': 0.05, 'hidden_layer_sizes': (28, 28), 'learning_rate': 'constant', 'solver': 'adam'}
0.931 (+/-0.008)	{'activation': 'tanh', 'alpha': 0.05, 'hidden_layer_sizes': (28, 28), 'learning_rate': 'adaptive', 'solver': 'sgd'}
0.948 (+/-0.007)	{'activation': 'tanh', 'alpha': 0.05, 'hidden_layer_sizes': (28, 28), 'learning_rate': 'adaptive', 'solver': 'adam'}
0.915 (+/-0.013)	{'activation': 'tanh', 'alpha': 0.05, 'hidden_layer_sizes': (784,), 'learning_rate': 'constant', 'solver': 'sgd'}
0.957 (+/-0.004)	{'activation': 'tanh', 'alpha': 0.05, 'hidden_layer_sizes': (784,), 'learning_rate': 'constant', 'solver': 'adam'}
0.914 (+/-0.013)	{'activation': 'tanh', 'alpha': 0.05, 'hidden_layer_sizes': (784,), 'learning_rate': 'adaptive', 'solver': 'sgd'}
0.958 (+/-0.003)	{'activation': 'tanh', 'alpha': 0.05, 'hidden_layer_sizes': (784,), 'learning_rate': 'adaptive', 'solver': 'adam'}
0.933 (+/-0.012)	{'activation': 'relu', 'alpha': 0.0001, 'hidden_layer_sizes': (28, 28), 'learning_rate': 'constant', 'solver': 'sgd'}
0.938 (+/-0.010)	{'activation': 'relu', 'alpha': 0.0001, 'hidden_layer_sizes': (28, 28), 'learning_rate': 'constant', 'solver': 'adam'}
0.930 (+/-0.011)	{'activation': 'relu', 'alpha': 0.0001, 'hidden_layer_sizes': (28, 28), 'learning_rate': 'adaptive', 'solver': 'sgd'}
0.938 (+/-0.006)	{'activation': 'relu', 'alpha': 0.0001, 'hidden_layer_sizes': (28, 28), 'learning_rate': 'adaptive', 'solver': 'adam'}
0.937 (+/-0.010)	{'activation': 'relu', 'alpha': 0.0001, 'hidden_layer_sizes': (784,), 'learning_rate': 'constant', 'solver': 'sgd'}
0.963 (+/-0.004)	{'activation': 'relu', 'alpha': 0.0001, 'hidden_layer_sizes': (784,), 'learning_rate': 'constant', 'solver': 'adam'}
0.937 (+/-0.012)	{'activation': 'relu', 'alpha': 0.0001, 'hidden_layer_sizes': (784,), 'learning_rate': 'adaptive', 'solver': 'sgd'}
0.963 (+/-0.004)	{'activation': 'relu', 'alpha': 0.0001, 'hidden_layer_sizes': (784,), 'learning_rate': 'adaptive', 'solver': 'adam'}
0.931 (+/-0.013)	{'activation': 'relu', 'alpha': 0.05, 'hidden_layer_sizes': (28, 28), 'learning_rate': 'constant', 'solver': 'sgd'}
0.941 (+/-0.010)	{'activation': 'relu', 'alpha': 0.05, 'hidden_layer_sizes': (28, 28), 'learning_rate': 'constant', 'solver': 'adam'}
0.931 (+/-0.010)	{'activation': 'relu', 'alpha': 0.05, 'hidden_layer_sizes': (28, 28), 'learning_rate': 'adaptive', 'solver': 'sgd'}
0.944 (+/-0.004)	{'activation': 'relu', 'alpha': 0.05, 'hidden_layer_sizes': (28, 28), 'learning_rate': 'adaptive', 'solver': 'adam'}
0.936 (+/-0.011)	{'activation': 'relu', 'alpha': 0.05, 'hidden_layer_sizes': (784,), 'learning_rate': 'constant', 'solver': 'sgd'}
0.965 (+/-0.005)	{'activation': 'relu', 'alpha': 0.05, 'hidden_layer_sizes': (784,), 'learning_rate': 'constant', 'solver': 'adam'}
0.937 (+/-0.011)	{'activation': 'relu', 'alpha': 0.05, 'hidden_layer_sizes': (784,), 'learning_rate': 'adaptive', 'solver': 'sgd'}
0.965 (+/-0.004)	{'activation': 'relu', 'alpha': 0.05, 'hidden_layer_sizes': (784,), 'learning_rate': 'adaptive', 'solver': 'adam'}
              precision    recall  f1-score   support

         0.0       0.96      0.99      0.97       219
         1.0       0.98      0.98      0.98       287
         2.0       0.97      0.95      0.96       276
         3.0       0.94      0.99      0.96       254
         4.0       0.97      0.95      0.96       275
         5.0       0.98      0.96      0.97       221
         6.0       0.96      0.95      0.96       225
         7.0       0.94      0.95      0.94       256
         8.0       0.96      0.95      0.95       242
         9.0       0.93      0.94      0.93       244

    accuracy                           0.96      2499
   macro avg       0.96      0.96      0.96      2499
weighted avg       0.96      0.96      0.96      2499

--- [2022-09-13 03:12:40.021741] Running Parameter-Tests [SKLEARN-NN-PCA] ---
	Best parameters set found on following development set:
		Support Vector: MLPClassifier(alpha=0.05, hidden_layer_sizes=(784,), learning_rate='adaptive',
              max_iter=100)
		Support Vector Parametrization: {'activation': 'relu', 'alpha': 0.05, 'hidden_layer_sizes': (784,), 'learning_rate': 'adaptive', 'solver': 'adam'}
		Asserted Score: 0.9667974880515727
Total Score 		 Configurations
0.914 (+/-0.012)	{'activation': 'tanh', 'alpha': 0.0001, 'hidden_layer_sizes': (28, 28), 'learning_rate': 'constant', 'solver': 'sgd'}
0.915 (+/-0.012)	{'activation': 'tanh', 'alpha': 0.0001, 'hidden_layer_sizes': (28, 28), 'learning_rate': 'constant', 'solver': 'adam'}
0.914 (+/-0.013)	{'activation': 'tanh', 'alpha': 0.0001, 'hidden_layer_sizes': (28, 28), 'learning_rate': 'adaptive', 'solver': 'sgd'}
0.915 (+/-0.010)	{'activation': 'tanh', 'alpha': 0.0001, 'hidden_layer_sizes': (28, 28), 'learning_rate': 'adaptive', 'solver': 'adam'}
0.907 (+/-0.014)	{'activation': 'tanh', 'alpha': 0.0001, 'hidden_layer_sizes': (784,), 'learning_rate': 'constant', 'solver': 'sgd'}
0.904 (+/-0.012)	{'activation': 'tanh', 'alpha': 0.0001, 'hidden_layer_sizes': (784,), 'learning_rate': 'constant', 'solver': 'adam'}
0.906 (+/-0.013)	{'activation': 'tanh', 'alpha': 0.0001, 'hidden_layer_sizes': (784,), 'learning_rate': 'adaptive', 'solver': 'sgd'}
0.906 (+/-0.012)	{'activation': 'tanh', 'alpha': 0.0001, 'hidden_layer_sizes': (784,), 'learning_rate': 'adaptive', 'solver': 'adam'}
0.915 (+/-0.013)	{'activation': 'tanh', 'alpha': 0.05, 'hidden_layer_sizes': (28, 28), 'learning_rate': 'constant', 'solver': 'sgd'}
0.934 (+/-0.008)	{'activation': 'tanh', 'alpha': 0.05, 'hidden_layer_sizes': (28, 28), 'learning_rate': 'constant', 'solver': 'adam'}
0.913 (+/-0.013)	{'activation': 'tanh', 'alpha': 0.05, 'hidden_layer_sizes': (28, 28), 'learning_rate': 'adaptive', 'solver': 'sgd'}
0.930 (+/-0.007)	{'activation': 'tanh', 'alpha': 0.05, 'hidden_layer_sizes': (28, 28), 'learning_rate': 'adaptive', 'solver': 'adam'}
0.907 (+/-0.015)	{'activation': 'tanh', 'alpha': 0.05, 'hidden_layer_sizes': (784,), 'learning_rate': 'constant', 'solver': 'sgd'}
0.955 (+/-0.004)	{'activation': 'tanh', 'alpha': 0.05, 'hidden_layer_sizes': (784,), 'learning_rate': 'constant', 'solver': 'adam'}
0.906 (+/-0.014)	{'activation': 'tanh', 'alpha': 0.05, 'hidden_layer_sizes': (784,), 'learning_rate': 'adaptive', 'solver': 'sgd'}
0.954 (+/-0.005)	{'activation': 'tanh', 'alpha': 0.05, 'hidden_layer_sizes': (784,), 'learning_rate': 'adaptive', 'solver': 'adam'}
0.923 (+/-0.010)	{'activation': 'relu', 'alpha': 0.0001, 'hidden_layer_sizes': (28, 28), 'learning_rate': 'constant', 'solver': 'sgd'}
0.927 (+/-0.004)	{'activation': 'relu', 'alpha': 0.0001, 'hidden_layer_sizes': (28, 28), 'learning_rate': 'constant', 'solver': 'adam'}
0.924 (+/-0.010)	{'activation': 'relu', 'alpha': 0.0001, 'hidden_layer_sizes': (28, 28), 'learning_rate': 'adaptive', 'solver': 'sgd'}
0.923 (+/-0.006)	{'activation': 'relu', 'alpha': 0.0001, 'hidden_layer_sizes': (28, 28), 'learning_rate': 'adaptive', 'solver': 'adam'}
0.930 (+/-0.013)	{'activation': 'relu', 'alpha': 0.0001, 'hidden_layer_sizes': (784,), 'learning_rate': 'constant', 'solver': 'sgd'}
0.949 (+/-0.007)	{'activation': 'relu', 'alpha': 0.0001, 'hidden_layer_sizes': (784,), 'learning_rate': 'constant', 'solver': 'adam'}
0.931 (+/-0.012)	{'activation': 'relu', 'alpha': 0.0001, 'hidden_layer_sizes': (784,), 'learning_rate': 'adaptive', 'solver': 'sgd'}
0.948 (+/-0.006)	{'activation': 'relu', 'alpha': 0.0001, 'hidden_layer_sizes': (784,), 'learning_rate': 'adaptive', 'solver': 'adam'}
0.924 (+/-0.012)	{'activation': 'relu', 'alpha': 0.05, 'hidden_layer_sizes': (28, 28), 'learning_rate': 'constant', 'solver': 'sgd'}
0.939 (+/-0.007)	{'activation': 'relu', 'alpha': 0.05, 'hidden_layer_sizes': (28, 28), 'learning_rate': 'constant', 'solver': 'adam'}
0.924 (+/-0.011)	{'activation': 'relu', 'alpha': 0.05, 'hidden_layer_sizes': (28, 28), 'learning_rate': 'adaptive', 'solver': 'sgd'}
0.939 (+/-0.006)	{'activation': 'relu', 'alpha': 0.05, 'hidden_layer_sizes': (28, 28), 'learning_rate': 'adaptive', 'solver': 'adam'}
0.931 (+/-0.012)	{'activation': 'relu', 'alpha': 0.05, 'hidden_layer_sizes': (784,), 'learning_rate': 'constant', 'solver': 'sgd'}
0.966 (+/-0.002)	{'activation': 'relu', 'alpha': 0.05, 'hidden_layer_sizes': (784,), 'learning_rate': 'constant', 'solver': 'adam'}
0.932 (+/-0.012)	{'activation': 'relu', 'alpha': 0.05, 'hidden_layer_sizes': (784,), 'learning_rate': 'adaptive', 'solver': 'sgd'}
0.967 (+/-0.003)	{'activation': 'relu', 'alpha': 0.05, 'hidden_layer_sizes': (784,), 'learning_rate': 'adaptive', 'solver': 'adam'}
              precision    recall  f1-score   support

         0.0       0.96      0.99      0.97       219
         1.0       0.97      0.98      0.97       287
         2.0       0.96      0.96      0.96       276
         3.0       0.96      0.98      0.97       254
         4.0       0.96      0.95      0.95       275
         5.0       0.99      0.94      0.96       221
         6.0       0.97      0.95      0.96       225
         7.0       0.96      0.92      0.94       256
         8.0       0.94      0.95      0.95       242
         9.0       0.93      0.95      0.94       244

    accuracy                           0.96      2499
   macro avg       0.96      0.96      0.96      2499
weighted avg       0.96      0.96      0.96      2499

